{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart Text Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9CsBIwvwOk/pabuTE3Ymo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhavaniShanker999/Djaango/blob/master/Smart_Text_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPHLUjMHE1YA"
      },
      "source": [
        "                      \r\n",
        "                      **Python code : Smart word-prediction for a sentence **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrqB6k8kLrjV"
      },
      "source": [
        "# Libraries\r\n",
        "from numpy import array\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense,LSTM,Embedding\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFzn2RKaTRBr"
      },
      "source": [
        "# Text on which our model is getting trained and \r\n",
        "data = \"\"\"[Today’s Date]\r\n",
        "[341 Company Address]\r\n",
        "Company City, State xxxxx\r\n",
        "(xxx) xxx-xxxx\r\n",
        "[hiring.manager@gmail.com]\r\n",
        "Dear Mr./Mrs./Ms. (Manager’s Name),\r\n",
        "I’m contacting you regarding your advertisement for the Human Resources opening listed on your website. My interest in this position stems from my belief that I have the right combination of relevant staffing experience, communication skills, and high levels of organization that make me a superb candidate.\r\n",
        "To date I feel my strongest abilities are:\r\n",
        "Increasing employee retention by rigorously maintaining a positive work environment\r\n",
        "Developing targeted outreach recruitment programs to recruit the best talent and meet all departmental hiring requirements\r\n",
        "Creating user-friendly application forms and questionnaires to be used by the organization during staff recruitment and interviewing.\r\n",
        "Arbitrating labor disputes in collaboration with the legal department.\r\n",
        "I consider myself to be a dedicated and dependable individual who possesses excellent verbal and written communication skills. I feel that a relationship with your company would be mutually beneficial, as my educational background, HR experience, and qualifications would make me a perfect fit for your Human Resources position, and would also allow me to refine my skills in a new working environment.\r\n",
        "In closing, I would like to thank you for your time and attention, and I hope to have the chance to discuss the opening with you in person.\r\n",
        "\r\n",
        "Sincerely,\"\"\"\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vuQ1AWlT4yl"
      },
      "source": [
        "# encoding the text[data] into integers\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts([data])\r\n",
        "encoded_data = tokenizer.texts_to_sequences([data])[0]\r\n",
        "# encoded_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9yrvUhbUsnv",
        "outputId": "efc0d51f-7fd2-4f97-a0a4-1aeadbdd38f6"
      },
      "source": [
        "# get number of different words present in text[vocabulary size]\r\n",
        "vocab_size = len(tokenizer.word_index)+1 \r\n",
        "# 1 is added because the 0 is reserved for padding\r\n",
        "print(\"vocab_size: \",vocab_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size:  143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ0xlJONVp0Q",
        "outputId": "1d8c9423-4959-4de1-dc82-32c353a18c16"
      },
      "source": [
        "# code to predict the next word and create sequences with input and output word\r\n",
        "sequences = []\r\n",
        "for i in range(1,len(encoded_data)):\r\n",
        "  sequence = encoded_data[i-1:i+1]\r\n",
        "  sequences.append(sequence)\r\n",
        "print(\"Total sequences list\",len(sequences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sequences list 221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVdEDj-JWjBZ",
        "outputId": "1c932dd4-d1ec-4f6f-b7bd-fc5baca43dc7"
      },
      "source": [
        "sequences = array(sequences)\r\n",
        "sequences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 35,  18],\n",
              "       [ 18,  36],\n",
              "       [ 36,  10],\n",
              "       [ 10,  37],\n",
              "       [ 37,  10],\n",
              "       [ 10,  38],\n",
              "       [ 38,  39],\n",
              "       [ 39,  40],\n",
              "       [ 40,  19],\n",
              "       [ 19,  19],\n",
              "       [ 19,  41],\n",
              "       [ 41,  20],\n",
              "       [ 20,  42],\n",
              "       [ 42,  43],\n",
              "       [ 43,  44],\n",
              "       [ 44,  45],\n",
              "       [ 45,  46],\n",
              "       [ 46,  47],\n",
              "       [ 47,  48],\n",
              "       [ 48,  49],\n",
              "       [ 49,  50],\n",
              "       [ 50,  51],\n",
              "       [ 51,  52],\n",
              "       [ 52,  11],\n",
              "       [ 11,  53],\n",
              "       [ 53,   6],\n",
              "       [  6,  54],\n",
              "       [ 54,  12],\n",
              "       [ 12,   3],\n",
              "       [  3,  21],\n",
              "       [ 21,  22],\n",
              "       [ 22,  23],\n",
              "       [ 23,  55],\n",
              "       [ 55,  56],\n",
              "       [ 56,   6],\n",
              "       [  6,  57],\n",
              "       [ 57,   7],\n",
              "       [  7,  58],\n",
              "       [ 58,   8],\n",
              "       [  8,  59],\n",
              "       [ 59,  24],\n",
              "       [ 24,  60],\n",
              "       [ 60,  61],\n",
              "       [ 61,   7],\n",
              "       [  7,  62],\n",
              "       [ 62,  13],\n",
              "       [ 13,   4],\n",
              "       [  4,  25],\n",
              "       [ 25,   3],\n",
              "       [  3,  63],\n",
              "       [ 63,  64],\n",
              "       [ 64,  26],\n",
              "       [ 26,  65],\n",
              "       [ 65,  66],\n",
              "       [ 66,  27],\n",
              "       [ 27,  28],\n",
              "       [ 28,  14],\n",
              "       [ 14,   1],\n",
              "       [  1,  67],\n",
              "       [ 67,  68],\n",
              "       [ 68,  26],\n",
              "       [ 26,  29],\n",
              "       [ 29,  13],\n",
              "       [ 13,  30],\n",
              "       [ 30,  15],\n",
              "       [ 15,   5],\n",
              "       [  5,  69],\n",
              "       [ 69,  70],\n",
              "       [ 70,   2],\n",
              "       [  2,  18],\n",
              "       [ 18,   4],\n",
              "       [  4,  31],\n",
              "       [ 31,   7],\n",
              "       [  7,  71],\n",
              "       [ 71,  72],\n",
              "       [ 72,  73],\n",
              "       [ 73,  74],\n",
              "       [ 74,  75],\n",
              "       [ 75,  76],\n",
              "       [ 76,  32],\n",
              "       [ 32,  77],\n",
              "       [ 77,  78],\n",
              "       [ 78,   5],\n",
              "       [  5,  79],\n",
              "       [ 79,  80],\n",
              "       [ 80,  33],\n",
              "       [ 33,  81],\n",
              "       [ 81,  82],\n",
              "       [ 82,  83],\n",
              "       [ 83,  34],\n",
              "       [ 34,  84],\n",
              "       [ 84,   2],\n",
              "       [  2,  85],\n",
              "       [ 85,   3],\n",
              "       [  3,  86],\n",
              "       [ 86,  87],\n",
              "       [ 87,   1],\n",
              "       [  1,  88],\n",
              "       [ 88,  89],\n",
              "       [ 89,  90],\n",
              "       [ 90,  20],\n",
              "       [ 20,  91],\n",
              "       [ 91,  92],\n",
              "       [ 92,  93],\n",
              "       [ 93,  94],\n",
              "       [ 94,  95],\n",
              "       [ 95,  96],\n",
              "       [ 96,   1],\n",
              "       [  1,  97],\n",
              "       [ 97,   2],\n",
              "       [  2,  16],\n",
              "       [ 16,  98],\n",
              "       [ 98,  32],\n",
              "       [ 32,   3],\n",
              "       [  3,  29],\n",
              "       [ 29,  99],\n",
              "       [ 99, 100],\n",
              "       [100,  34],\n",
              "       [ 34,   1],\n",
              "       [  1, 101],\n",
              "       [101, 102],\n",
              "       [102, 103],\n",
              "       [103, 104],\n",
              "       [104,   8],\n",
              "       [  8, 105],\n",
              "       [105,  17],\n",
              "       [ 17,   3],\n",
              "       [  3, 106],\n",
              "       [106, 107],\n",
              "       [107,   4],\n",
              "       [  4, 108],\n",
              "       [108, 109],\n",
              "       [109,   2],\n",
              "       [  2,  16],\n",
              "       [ 16,   5],\n",
              "       [  5, 110],\n",
              "       [110,   1],\n",
              "       [  1, 111],\n",
              "       [111, 112],\n",
              "       [112, 113],\n",
              "       [113, 114],\n",
              "       [114, 115],\n",
              "       [115, 116],\n",
              "       [116,   1],\n",
              "       [  1, 117],\n",
              "       [117,  28],\n",
              "       [ 28,  14],\n",
              "       [ 14,   4],\n",
              "       [  4,  31],\n",
              "       [ 31,  13],\n",
              "       [ 13,   5],\n",
              "       [  5, 118],\n",
              "       [118,  17],\n",
              "       [ 17,   6],\n",
              "       [  6,  10],\n",
              "       [ 10,   9],\n",
              "       [  9,  16],\n",
              "       [ 16, 119],\n",
              "       [119, 120],\n",
              "       [120, 121],\n",
              "       [121,   7],\n",
              "       [  7, 122],\n",
              "       [122, 123],\n",
              "       [123, 124],\n",
              "       [124,  27],\n",
              "       [ 27,   1],\n",
              "       [  1, 125],\n",
              "       [125,   9],\n",
              "       [  9,  30],\n",
              "       [ 30,  15],\n",
              "       [ 15,   5],\n",
              "       [  5, 126],\n",
              "       [126, 127],\n",
              "       [127,  12],\n",
              "       [ 12,   6],\n",
              "       [  6,  21],\n",
              "       [ 21,  22],\n",
              "       [ 22,  24],\n",
              "       [ 24,   1],\n",
              "       [  1,   9],\n",
              "       [  9, 128],\n",
              "       [128, 129],\n",
              "       [129,  15],\n",
              "       [ 15,   2],\n",
              "       [  2, 130],\n",
              "       [130,   7],\n",
              "       [  7,  14],\n",
              "       [ 14,   8],\n",
              "       [  8,   5],\n",
              "       [  5, 131],\n",
              "       [131, 132],\n",
              "       [132,  33],\n",
              "       [ 33,   8],\n",
              "       [  8, 133],\n",
              "       [133,   4],\n",
              "       [  4,   9],\n",
              "       [  9, 134],\n",
              "       [134,   2],\n",
              "       [  2, 135],\n",
              "       [135,  11],\n",
              "       [ 11,  12],\n",
              "       [ 12,   6],\n",
              "       [  6, 136],\n",
              "       [136,   1],\n",
              "       [  1, 137],\n",
              "       [137,   1],\n",
              "       [  1,   4],\n",
              "       [  4, 138],\n",
              "       [138,   2],\n",
              "       [  2,  25],\n",
              "       [ 25,   3],\n",
              "       [  3, 139],\n",
              "       [139,   2],\n",
              "       [  2, 140],\n",
              "       [140,   3],\n",
              "       [  3,  23],\n",
              "       [ 23,  17],\n",
              "       [ 17,  11],\n",
              "       [ 11,   8],\n",
              "       [  8, 141],\n",
              "       [141, 142]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aaD9JvqXduo"
      },
      "source": [
        "# splitting sequences into x and y\r\n",
        "x,y = sequences[:,0],sequences[:,1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxdD-WJSXrrL"
      },
      "source": [
        "y = to_categorical(y , num_classes= vocab_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBMwhsqsYWee",
        "outputId": "d4a95632-8352-4090-dd6e-b756f897ab10"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add((Embedding(vocab_size,10,input_length =1)))\r\n",
        "model.add(LSTM(50))\r\n",
        "model.add(Dense(vocab_size , activation = 'softmax'))\r\n",
        "print(model.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1, 10)             1430      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50)                12200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 143)               7293      \n",
            "=================================================================\n",
            "Total params: 20,923\n",
            "Trainable params: 20,923\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUoEY768Zcf1"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfY0Q10NZuKQ",
        "outputId": "b45d597b-fbb2-45fd-e709-2ced0d006a59"
      },
      "source": [
        "model.fit(x,y, epochs=100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 2s 3ms/step - loss: 4.9629 - accuracy: 0.0158 \n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9603 - accuracy: 0.0501\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9582 - accuracy: 0.0486\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 4.9553 - accuracy: 0.0562\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9523 - accuracy: 0.0710\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9501 - accuracy: 0.0677\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9449 - accuracy: 0.0509\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 5ms/step - loss: 4.9436 - accuracy: 0.0494\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.9391 - accuracy: 0.0441\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9299 - accuracy: 0.0714\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9220 - accuracy: 0.0493\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9193 - accuracy: 0.0363\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9128 - accuracy: 0.0425\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.9011 - accuracy: 0.0472\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.8862 - accuracy: 0.0372\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.8709 - accuracy: 0.0476\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.8569 - accuracy: 0.0472\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.8378 - accuracy: 0.0389\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.8017 - accuracy: 0.0552\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.7889 - accuracy: 0.0504\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.7549 - accuracy: 0.0403\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.7285 - accuracy: 0.0426\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.6861 - accuracy: 0.0435\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.6676 - accuracy: 0.0514\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 4.6091 - accuracy: 0.0275\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.5165 - accuracy: 0.0647\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 4.5422 - accuracy: 0.0359\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.4009 - accuracy: 0.0573\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.4622 - accuracy: 0.0427\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.4033 - accuracy: 0.0523\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.2766 - accuracy: 0.0661\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.3254 - accuracy: 0.0610\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.2182 - accuracy: 0.0723\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.1123 - accuracy: 0.0739\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.1622 - accuracy: 0.0741\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.0809 - accuracy: 0.0714\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.0717 - accuracy: 0.0769\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 4.0802 - accuracy: 0.0770\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.9701 - accuracy: 0.0798\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.9194 - accuracy: 0.0894\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 3.9168 - accuracy: 0.0962\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.9178 - accuracy: 0.0973\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 3.8445 - accuracy: 0.0855\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.8443 - accuracy: 0.0874\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 3.7644 - accuracy: 0.0941\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.7676 - accuracy: 0.0979\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.7871 - accuracy: 0.1104\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.7673 - accuracy: 0.0919\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.7577 - accuracy: 0.1085\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.6891 - accuracy: 0.1012\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 3.5899 - accuracy: 0.1597\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.5947 - accuracy: 0.1582\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.6233 - accuracy: 0.1322\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.5118 - accuracy: 0.1296\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.5331 - accuracy: 0.1620\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.4513 - accuracy: 0.1555\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.5113 - accuracy: 0.1467\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.4452 - accuracy: 0.1709\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.3196 - accuracy: 0.2089\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 3.3380 - accuracy: 0.1901\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 3.3883 - accuracy: 0.1573\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.3508 - accuracy: 0.1817\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.2891 - accuracy: 0.2256\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.2579 - accuracy: 0.2251\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.2349 - accuracy: 0.2364\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.2182 - accuracy: 0.2328\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 3.2004 - accuracy: 0.2341\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.1690 - accuracy: 0.2701\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.1166 - accuracy: 0.2412\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.1085 - accuracy: 0.2774\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.1362 - accuracy: 0.2650\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.0117 - accuracy: 0.3033\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 3.0614 - accuracy: 0.2766\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.9699 - accuracy: 0.3038\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.9437 - accuracy: 0.3188\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.9209 - accuracy: 0.3090\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.9164 - accuracy: 0.3211\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.8774 - accuracy: 0.3287\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.8982 - accuracy: 0.3175\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.8052 - accuracy: 0.3346\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.8431 - accuracy: 0.3258\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.8074 - accuracy: 0.3219\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.7643 - accuracy: 0.3437\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.7427 - accuracy: 0.3751\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.5799 - accuracy: 0.3963\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.7095 - accuracy: 0.3731\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.6424 - accuracy: 0.3493\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.5848 - accuracy: 0.4009\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.5861 - accuracy: 0.3879\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.6401 - accuracy: 0.3628\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.4760 - accuracy: 0.3954\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.5232 - accuracy: 0.3894\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.5506 - accuracy: 0.3761\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.4551 - accuracy: 0.4281\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.3588 - accuracy: 0.4262\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.4156 - accuracy: 0.4414\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 2.3387 - accuracy: 0.4616\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 4ms/step - loss: 2.3546 - accuracy: 0.4369\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.2659 - accuracy: 0.4699\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 2.3376 - accuracy: 0.4592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f74bf76d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JS3sep7aMoW"
      },
      "source": [
        "#generate a sequence for the word from the model\r\n",
        "def gen_seq(model , tokenizer , enter_text , n_pred):\r\n",
        "  in_text, result = enter_text,enter_text\r\n",
        "\r\n",
        "  for _ in range(n_pred):\r\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\r\n",
        "    encoded = array(encoded)\r\n",
        "\r\n",
        "    yhat = model.predict_classes(encoded)\r\n",
        "\r\n",
        "    out_word = ''\r\n",
        "    for word , index in tokenizer.word_index.items():\r\n",
        "      if index == yhat:\r\n",
        "        out_word = word\r\n",
        "        break\r\n",
        "    in_text , result  = out_word , result +' '+ out_word\r\n",
        "  return result\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFhKoehYb4ZV",
        "outputId": "f4adde9d-e942-4d63-806a-67db3b62ea7e"
      },
      "source": [
        "print(gen_seq(model , tokenizer ,'strongest',6))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "strongest abilities are increasing employee retention by\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}